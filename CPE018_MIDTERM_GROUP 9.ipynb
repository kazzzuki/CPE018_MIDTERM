{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d44fc7c-ab9b-43cc-919b-020cf9fce309",
   "metadata": {},
   "source": [
    "# CPE018 Midterm Exam (1st Sem, A.Y. 2023-2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c73f82-2df6-45d1-852f-70f4f7fe0b73",
   "metadata": {},
   "source": [
    "Student Submission Details:\n",
    "* Name: Kazuki Ogata\n",
    "* Section: CPE31S5\n",
    "* Schedule: Wednesday 1:30pm - 4:30pm\n",
    "* Instructor: Engr. Verlyn V. Nojor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600adeea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fed455-ea33-4e13-a725-0cd25e99041b",
   "metadata": {},
   "source": [
    "## Intended Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f5a5b-4ac1-40f8-9b24-d244e929a3d3",
   "metadata": {},
   "source": [
    "By the end of this activity, the student should be able to:\n",
    "* ILO1: Demonstrate different methods for feature matching and detection learned in class and indepdentently from new sources.\n",
    "* ILO2: Evaluate the accuracy of different feature matching and detection methods and scrutinize its applicability in solving a given real-life problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76385021",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9993c5-0f71-4bdc-8f4b-af50975b9d91",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8324f2-2f10-4a8b-873f-400c32338886",
   "metadata": {},
   "source": [
    "For this examination, you must create a **mood detection** program with an object-oriented programming approach (same as project CAMEO), it must detect mood changes through the use of algorithms/techniques/schemes learned in class, and from external sources.\n",
    "\n",
    "In this file, you have to include for each section of your solution your completion of the following:\n",
    "\n",
    "* Part 1: **Face Detection**: Once your face is detected using any algorithm, it must draw an ROI. The color for the ROI is your choice; however, it must detect for all faces in the frame and draw a corresponding ROI.\n",
    "* Part 2: **Face Recognition**: The detected face must then be recognized, using any of the provided tools in class, the ROIs must indicate whether it is your face or someone it doesn't recognize.\n",
    "* Part 3: **Mood Detection**: Use three different feature detection and matching techniques to determine three emotion: happy, sad and neutral. Two of the techniques must be learned from class, and 1 must be one you independently learned.\n",
    "\n",
    "Properly show through your notebook the output for each part of the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708ca1c-ed32-40fb-97bc-a017b4380692",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3b1fb-327f-4723-aea3-543f1221c683",
   "metadata": {},
   "source": [
    "## Procedure and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef60d7f-d688-4ead-b9db-36b3739cc3ff",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* This is the section where you have to include all  your answers to the items provided in the tasks section.\n",
    "* Tasks 1 and 2 contribute directly to ILO1: Demonstrate different methods for feature matching and detection learned in class and indepdentently from new sources.\n",
    "* Task 3 contributes directly to ILO2: Evaluate the accuracy of different feature matching and detection methods and scrutinize its applicability in solving a given real-life problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde1091-4711-4343-a48d-edad768437dd",
   "metadata": {},
   "source": [
    "### Task 1: Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13054d3d-fc5e-4e43-bdac-2f699165dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include your code here\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.3,\n",
    "        minNeighbors=5,      \n",
    "        minSize=(30, 30))\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),3)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "          \n",
    "        cv2.imshow('Part 1: Face Detection', img)\n",
    "\n",
    "    q = cv2.waitKey(30) & 0xff\n",
    "    if q == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ade5caf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1ZF1ITxFrrnZJ81SircIqxD1xbB2MMAEw\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1: Output\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://drive.google.com/uc?id=1ZF1ITxFrrnZJ81SircIqxD1xbB2MMAEw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd46e7b-4155-47de-9ada-a775afb39b2d",
   "metadata": {},
   "source": [
    "### Task 2: Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71ae16-2dd0-4171-8a69-4129338d695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include your code here\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "def gather_images():\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    facedetect = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    id = input(\"Enter Your ID: \")\n",
    "    uploadSize = input(\"How many captures: \")\n",
    "    count=0\n",
    "\n",
    "    while True:\n",
    "        ret,frame = camera.read()\n",
    "        gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = facedetect.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x,y,w,h) in faces:\n",
    "            count=count+1\n",
    "            cv2.imwrite('datasets/User.'+str(id)+\".\"+str(count)+\".jpg\", gray[y:y+h, x:x+w])\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (50,50,255), 3)\n",
    "            cv2.putText(frame, \"Analyzing \"+str(count), (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "        k=cv2.waitKey(1)\n",
    "        if count>int(uploadSize):\n",
    "            break\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def read_images(path, sz=None):\n",
    "  imagePath = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "  faces=[]\n",
    "  ids=[]\n",
    "  for imagePaths in imagePath:\n",
    "      faceImage = Image.open(imagePaths).convert('L')\n",
    "      faceNP = np.array(faceImage)\n",
    "      Id= (os.path.split(imagePaths)[-1].split(\".\")[1])\n",
    "      Id=int(Id)\n",
    "      faces.append(faceNP)\n",
    "      ids.append(Id)\n",
    "  return ids, faces\n",
    "\n",
    "def face_recognition():\n",
    "    names = [\"\", \"Kazuki A. Ogata\", \"John Renzo L. Mendoza\"] # Put your names here for faces to recognize\n",
    "  \n",
    "    camera = cv2.VideoCapture(0)\n",
    "    face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    path = \"datasets\"\n",
    "    \n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    IDs, facedata = read_images(path)\n",
    "    recognizer.train(facedata, np.array(IDs))\n",
    "    recognizer.write(\"Trainer.yml\")\n",
    "    recognizer.read(\"Trainer.yml\")\n",
    "  \n",
    "    while True:\n",
    "        ret,frame = camera.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x,y,w,h) in faces:\n",
    "            serial, conf = recognizer.predict(gray[y:y+h, x:x+w])\n",
    "            if conf<50:\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "                print(serial)\n",
    "                cv2.putText(frame, names[serial], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,0),2)\n",
    "            else:\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "                cv2.putText(frame, \"Neutral\", (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,255),2)\n",
    "\n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "        \n",
    "        k=cv2.waitKey(1)\n",
    "        \n",
    "        if k==ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        id = input(\"Import Images? (Y/N): \")\n",
    "        if id == 'Y' or id == 'y':\n",
    "            gather_images()\n",
    "        else:\n",
    "            break\n",
    "    face_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "855d8704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1lO_ESkXTMfBPvRjuEi_fqpixI3oYyr2Q\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2: Output\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://drive.google.com/uc?id=1lO_ESkXTMfBPvRjuEi_fqpixI3oYyr2Q')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5b345-0442-4e42-8500-a247a520383e",
   "metadata": {},
   "source": [
    "### Task 3: Mood Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc361ca0-d355-4470-9fba-2b3b67bb240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 1\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "def gather_images():\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    facedetect = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    id = input(\"Enter Your ID: \")\n",
    "    uploadSize = input(\"How many captures: \")\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret,frame = camera.read()\n",
    "        gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = facedetect.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x,y,w,h) in faces:\n",
    "            count=count+1\n",
    "            cv2.imwrite('emoteSet/User.'+str(id)+\".\"+str(count)+\".jpg\", gray[y:y+h, x:x+w])\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), (50,50,255), 3)\n",
    "            cv2.putText(frame, \"Analyzing \"+str(count), (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "        k=cv2.waitKey(1)\n",
    "        if count>int(uploadSize):\n",
    "            break\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def read_images(path, sz=None):\n",
    "  imagePath = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "  faces=[]\n",
    "  ids=[]\n",
    "  for imagePaths in imagePath:\n",
    "      faceImage = Image.open(imagePaths).convert('L')\n",
    "      faceNP = np.array(faceImage)\n",
    "      Id= (os.path.split(imagePaths)[-1].split(\".\")[1])\n",
    "      Id=int(Id)\n",
    "      faces.append(faceNP)\n",
    "      ids.append(Id)\n",
    "  return ids, faces\n",
    "\n",
    "def face_recognition():\n",
    "    names = [\"\", \"Happy\", \"Sad\"] # Put your names here for faces to recognize\n",
    "  \n",
    "    camera = cv2.VideoCapture(0)\n",
    "    face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    path = \"emoteSet\"\n",
    "    \n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    IDs, facedata = read_images(path)\n",
    "    recognizer.train(facedata, np.array(IDs))\n",
    "    recognizer.write(\"Trainer.yml\")\n",
    "    recognizer.read(\"Trainer.yml\")\n",
    "  \n",
    "    while True:\n",
    "        ret,frame = camera.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x,y,w,h) in faces:\n",
    "            serial, conf = recognizer.predict(gray[y:y+h, x:x+w])\n",
    "            if conf<50:\n",
    "                if serial == 1:\n",
    "                    cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "                    cv2.putText(frame, names[serial], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,0),2)\n",
    "                if serial == 2:\n",
    "                    cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "                    cv2.putText(frame, names[serial], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,0,0),2)\n",
    "            else:\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "                cv2.putText(frame, \"Neutral\", (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,255),2)\n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "        k=cv2.waitKey(1)\n",
    "        if k==ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        id = input(\"Import Images? (Y/N): \")\n",
    "        if id == 'Y' or id == 'y':\n",
    "            gather_images()\n",
    "        else:\n",
    "            break\n",
    "    face_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01e038d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1NqWIgHSRV7BlOyo1C_0DbtwqJox3tZUG\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 3 - Technique 1: Output (10x test)\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://drive.google.com/uc?id=1NqWIgHSRV7BlOyo1C_0DbtwqJox3tZUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb965e0c-a349-4e1c-b51d-e21db970dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Trainer(number):\n",
    "    output_folder = 'emoteSet2/'+str(number)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier(f'{cv2.data.haarcascades}haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier(f'{cv2.data.haarcascades}haarcascade_eye.xml')\n",
    "    \n",
    "    camera = cv2.VideoCapture(0)\n",
    "    count = 0\n",
    "    \n",
    "    while (cv2.waitKey(1) == -1):\n",
    "        success, frame = camera.read()\n",
    "        \n",
    "        if success:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray, 1.3, 5, minSize=(120, 120))\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "                face_img = cv2.resize(gray[y:y+h, x:x+w], (200, 200))\n",
    "                face_filename = '%s/%d.pgm' % (output_folder, count)\n",
    "                cv2.imwrite(face_filename, face_img)\n",
    "                count += 1\n",
    "            if count >= 100:\n",
    "                break\n",
    "            cv2.imshow('Capturing Faces...', frame)\n",
    "            \n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def Generate():\n",
    "    imageDirectory = \"emoteSet2\"\n",
    "\n",
    "    image_array = []\n",
    "    label_array = []\n",
    "\n",
    "    label_IDs = [\"1\", \"2\"]\n",
    "\n",
    "    for ID in label_IDs:\n",
    "        dataset = os.path.join(imageDirectory, ID)\n",
    "        \n",
    "        for filename in os.listdir(dataset):\n",
    "            image_path = os.path.join(dataset, filename)\n",
    "            \n",
    "            if filename.endswith(\".jpg\"): \n",
    "                img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                pgm_path = os.path.splitext(image_path)[0] + \".pgm\"\n",
    "                cv2.imwrite(pgm_path, img)\n",
    "            \n",
    "            if filename.endswith(\".pgm\"):         \n",
    "                image_array.append(image_path)\n",
    "                label_array.append(ID)\n",
    "\n",
    "    data = {\n",
    "        'Image Path': image_array, \n",
    "        'Label': label_array\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    csv_file = \"emoteSetData2.csv\"\n",
    "    df.to_csv(csv_file, index = False)\n",
    "\n",
    "def read_images(path):\n",
    "    X, y = [], []\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        filepath, label = str(row['Image Path']), int(row['Label'])\n",
    "        im = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        im = cv2.resize(im, (200, 200))\n",
    "        X.append(np.asarray(im, dtype=np.uint8))\n",
    "        y.append(label)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def face_recognition():\n",
    "    # Names used to display on face recognition\n",
    "    names = ['', 'Happy', 'Sad']\n",
    "\n",
    "    # Input Target CSV File to learn\n",
    "    [X, y] = read_images('emoteSetData2.csv')\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "    # Procedure\n",
    "    model = cv2.face.LBPHFaceRecognizer_create()\n",
    "    model.train(X, y)\n",
    "\n",
    "    # Open Web Camera\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Processing Frames from Web Camera\n",
    "    while True:\n",
    "        ret, img = camera.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(grayImage, 1.3, 5)\n",
    "\n",
    "        # Recognize Faces present on Web Camera\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            gray = cv2.cvtColor(img[y:y + h, x:x + w], cv2.COLOR_BGR2GRAY)\n",
    "            roi = cv2.resize(gray, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Face Recognition based on Learned Data\n",
    "            params = model.predict(roi)\n",
    "            print (params[0])\n",
    "\n",
    "            # Recognize the person based on set parameters\n",
    "            if params[0] == 1:\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (0, 255, 0), 3)\n",
    "                cv2.putText(img, names[1] + \"Emotion\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif params[0] == 2:\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0, 0), 3)\n",
    "                cv2.putText(img, names[2] + \"Emotion\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            else:\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (0, 0, 255), 3)\n",
    "                cv2.putText(img, 'Neutral' + \", \" + str(params[1]), (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Creation of a Web Camera Window\n",
    "        cv2.imshow(\"Face Recognition\", img)\n",
    "\n",
    "        # Escape Key to break from the Loop / Close Web Camera Window\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # Closing the Web Camera Window\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        choice = input(\"Import Images? (Y/N): \")\n",
    "        if choice == 'Y' or choice == 'y':\n",
    "            id = input(\"Enter your ID: \")\n",
    "            Trainer(id)\n",
    "        else:\n",
    "            Generate()\n",
    "            break\n",
    "    face_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd50cc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1YQn7h_8ssx0lP74pOoXXyBq7iJ6ngnYd\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 3 - Technique 2: Output (10x test)\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://drive.google.com/uc?id=1YQn7h_8ssx0lP74pOoXXyBq7iJ6ngnYd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46261895-0b3f-4cfa-b62e-46871a91c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 3\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Trainer(number):\n",
    "    output_folder = 'emoteSet2/'+str(number)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier(f'{cv2.data.haarcascades}haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier(f'{cv2.data.haarcascades}haarcascade_eye.xml')\n",
    "    \n",
    "    camera = cv2.VideoCapture(0)\n",
    "    count = 0\n",
    "    \n",
    "    while (cv2.waitKey(1) == -1):\n",
    "        success, frame = camera.read()\n",
    "        \n",
    "        if success:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray, 1.3, 5, minSize=(120, 120))\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "                face_img = cv2.resize(gray[y:y+h, x:x+w], (200, 200))\n",
    "                face_filename = '%s/%d.pgm' % (output_folder, count)\n",
    "                cv2.imwrite(face_filename, face_img)\n",
    "                count += 1\n",
    "            if count >= 100:\n",
    "                break\n",
    "            cv2.imshow('Capturing Faces...', frame)\n",
    "            \n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def Generate():\n",
    "    imageDirectory = \"emoteSet2\"\n",
    "\n",
    "    image_array = []\n",
    "    label_array = []\n",
    "\n",
    "    label_IDs = [\"1\", \"2\"]\n",
    "\n",
    "    for ID in label_IDs:\n",
    "        dataset = os.path.join(imageDirectory, ID)\n",
    "        \n",
    "        for filename in os.listdir(dataset):\n",
    "            image_path = os.path.join(dataset, filename)\n",
    "            \n",
    "            if filename.endswith(\".jpg\"): \n",
    "                img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                pgm_path = os.path.splitext(image_path)[0] + \".pgm\"\n",
    "                cv2.imwrite(pgm_path, img)\n",
    "            \n",
    "            if filename.endswith(\".pgm\"):         \n",
    "                image_array.append(image_path)\n",
    "                label_array.append(ID)\n",
    "\n",
    "    data = {\n",
    "        'Image Path': image_array, \n",
    "        'Label': label_array\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    csv_file = \"emoteSetData2.csv\"\n",
    "    df.to_csv(csv_file, index = False)\n",
    "\n",
    "def read_images(path):\n",
    "    X, y = [], []\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        filepath, label = str(row['Image Path']), int(row['Label'])\n",
    "        im = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        im = cv2.resize(im, (200, 200))\n",
    "        X.append(np.asarray(im, dtype=np.uint8))\n",
    "        y.append(label)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def face_recognition():\n",
    "    # Names used to display on face recognition\n",
    "    names = ['', 'Smile', 'Sad']\n",
    "\n",
    "    # Input Target CSV File to learn\n",
    "    [X, y] = read_images('emoteSetData2.csv')\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "    # Procedure\n",
    "    model = cv2.face.EigenFaceRecognizer_create()\n",
    "    model.train(X, y)\n",
    "\n",
    "    # Open Web Camera\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Processing Frames from Web Camera\n",
    "    while True:\n",
    "        ret, img = camera.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(grayImage, 1.3, 5)\n",
    "\n",
    "        # Recognize Faces present on Web Camera\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            gray = cv2.cvtColor(img[y:y + h, x:x + w], cv2.COLOR_BGR2GRAY)\n",
    "            roi = cv2.resize(gray, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Face Recognition based on Learned Data\n",
    "            params = model.predict(roi)\n",
    "            print (params[0])\n",
    "\n",
    "            # Recognize the person based on set parameters\n",
    "            if params[0] == 1:\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (0, 255, 0), 3)\n",
    "                cv2.putText(img, names[1] + \"Emotion\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            elif params[0] == 2:\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0, 0), 3)\n",
    "                cv2.putText(img, names[2] + \"Emotion\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            else:\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), (0, 0, 255), 3)\n",
    "                cv2.putText(img, 'Not Recognized' + \", \" + str(params[1]), (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Creation of a Web Camera Window\n",
    "        cv2.imshow(\"Face Recognition\", img)\n",
    "\n",
    "        # Escape Key to break from the Loop / Close Web Camera Window\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # Closing the Web Camera Window\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        choice = input(\"Import Images? (Y/N): \")\n",
    "        if choice == 'Y' or choice == 'y':\n",
    "            id = input(\"Enter your ID: \")\n",
    "            Trainer(id)\n",
    "        else:\n",
    "            Generate()\n",
    "            break\n",
    "    face_recognition()\n",
    "    results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "546cea98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1ruOIrG80XHgG6vwVKwNSFywocBHnG293\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 3 - Technique 3: Output (10x test)\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://drive.google.com/uc?id=1ruOIrG80XHgG6vwVKwNSFywocBHnG293')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b537d4-927d-4adf-84e2-3fe4a7fef365",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c97cb-2d49-4de4-9067-04245f9ab9cf",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c6947-908c-4dd2-89c7-7dc4bb2f58f8",
   "metadata": {},
   "source": [
    "For the three different techniques you used in face detection, provide an in-depth analysis. \n",
    "\n",
    "To do this, you must:\n",
    "* Test the face detection, face recongition, and mood detection functions 10 times each. Only the mood detection will have components for 10 tests for each different technique used.\n",
    "* Create a table containing the 10 tests (like shown below) for each task.\n",
    "* Analyze each output by identifying the accuracy and providing your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b079aab-6780-455a-9b69-b779028a02a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: Mood Detection (FIRST TECHNIQUE)\n",
      "+----------+------------+----------+---------+\n",
      "|   Test # | Expected   | Actual   |   Score |\n",
      "+==========+============+==========+=========+\n",
      "|        1 | Neutral    | Neutral  |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        2 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        3 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        4 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        5 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        6 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        7 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        8 | Neutral    | Happy    |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "|        9 | Neutral    | Sad      |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "|       10 | Neutral    | Happy    |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "Accuracy:  70.0\n"
     ]
    }
   ],
   "source": [
    "# Import the module for tabulating the data\n",
    "from tabulate import tabulate\n",
    " \n",
    "# Create a list for content of the table\n",
    "test_results = [\n",
    "    [\"1\", \"Neutral\", \"Neutral\", 1],\n",
    "    [\"2\", \"Happy\", \"Happy\", 1],\n",
    "    [\"3\", \"Sad\", \"Sad\", 1],\n",
    "    [\"4\", \"Happy\", \"Happy\", 1],\n",
    "    [\"5\", \"Sad\", \"Sad\", 1],\n",
    "    [\"6\", \"Happy\", \"Happy\", 1],\n",
    "    [\"7\", \"Sad\", \"Sad\", 1],\n",
    "    [\"8\", \"Neutral\", \"Happy\", 0],\n",
    "    [\"9\", \"Neutral\", \"Sad\", 0],\n",
    "    [\"10\", \"Neutral\", \"Happy\", 0],\n",
    "]\n",
    " \n",
    "# Create a list for the headers of your table\n",
    "header = [\"Test #\", \"Expected\", \"Actual\", \"Score\"]\n",
    " \n",
    "# display table\n",
    "print(\"Task 3: Mood Detection (FIRST TECHNIQUE)\")\n",
    "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
    "\n",
    "# Calculate for the accuracy\n",
    "total = 0\n",
    "for i in test_results:\n",
    "    total += i[3]\n",
    "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7610139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: Mood Detection (SECOND TECHNIQUE)\n",
      "+----------+------------+----------+---------+\n",
      "|   Test # | Expected   | Actual   |   Score |\n",
      "+==========+============+==========+=========+\n",
      "|        1 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        2 | Happy      | Sad      |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "|        3 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        4 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        5 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        6 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        7 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        8 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        9 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|       10 | Neutral    | Happy    |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "Accuracy:  80.0\n"
     ]
    }
   ],
   "source": [
    "# Import the module for tabulating the data\n",
    "from tabulate import tabulate\n",
    " \n",
    "# Create a list for content of the table\n",
    "test_results = [\n",
    "    [\"1\", \"Happy\", \"Happy\", 1],\n",
    "    [\"2\", \"Happy\", \"Sad\", 0],\n",
    "    [\"3\", \"Happy\", \"Happy\", 1],\n",
    "    [\"4\", \"Happy\", \"Happy\", 1],\n",
    "    [\"5\", \"Happy\", \"Happy\", 1],\n",
    "    [\"6\", \"Sad\", \"Sad\", 1],\n",
    "    [\"7\", \"Sad\", \"Sad\", 1],\n",
    "    [\"8\", \"Sad\", \"Sad\", 1],\n",
    "    [\"9\", \"Sad\", \"Sad\", 1],\n",
    "    [\"10\", \"Neutral\", \"Happy\", 0],\n",
    "]\n",
    " \n",
    "# Create a list for the headers of your table\n",
    "header = [\"Test #\", \"Expected\", \"Actual\", \"Score\"]\n",
    " \n",
    "# display table\n",
    "print(\"Task 3: Mood Detection (SECOND TECHNIQUE)\")\n",
    "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
    "\n",
    "# Calculate for the accuracy\n",
    "total = 0\n",
    "for i in test_results:\n",
    "    total += i[3]\n",
    "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e98f74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: Mood Detection (THIRD TECHNIQUE)\n",
      "+----------+------------+----------+---------+\n",
      "|   Test # | Expected   | Actual   |   Score |\n",
      "+==========+============+==========+=========+\n",
      "|        1 | Neutral    | Happy    |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "|        2 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        3 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        4 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        5 | Happy      | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        6 | Neutral    | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        7 | Neutral    | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        8 | Happy      | Happy    |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "|        9 | Sad        | Sad      |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "|       10 | Sad        | Happy    |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "Accuracy:  60.0\n"
     ]
    }
   ],
   "source": [
    "# Import the module for tabulating the data\n",
    "from tabulate import tabulate\n",
    " \n",
    "# Create a list for content of the table\n",
    "test_results = [\n",
    "    [\"1\", \"Neutral\", \"Happy\", 0],\n",
    "    [\"2\", \"Happy\", \"Happy\", 1],\n",
    "    [\"3\", \"Sad\", \"Sad\", 1],\n",
    "    [\"4\", \"Happy\", \"Happy\", 1],\n",
    "    [\"5\", \"Happy\", \"Sad\", 1],\n",
    "    [\"6\", \"Neutral\", \"Happy\", 1],\n",
    "    [\"7\", \"Neutral\", \"Sad\", 1],\n",
    "    [\"8\", \"Happy\", \"Happy\", 0],\n",
    "    [\"9\", \"Sad\", \"Sad\", 0],\n",
    "    [\"10\", \"Sad\", \"Happy\", 0],\n",
    "]\n",
    " \n",
    "# Create a list for the headers of your table\n",
    "header = [\"Test #\", \"Expected\", \"Actual\", \"Score\"]\n",
    " \n",
    "# display table\n",
    "print(\"Task 3: Mood Detection (THIRD TECHNIQUE)\")\n",
    "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
    "\n",
    "# Calculate for the accuracy\n",
    "total = 0\n",
    "for i in test_results:\n",
    "    total += i[3]\n",
    "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499044b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94907d",
   "metadata": {},
   "source": [
    "Accuracy Summary:\n",
    "\n",
    "Technique 1: 70%\n",
    "- This technique quite performed well, it correctly identified different mood 7 out of 10. The wrong mood detection occured in the Neutral mood, since this technique keeps showing Happy or Sad emotions.\n",
    "\n",
    "Technique 2: 80%\n",
    "- This technique is the best among the 3 techiniques we used, it correctly detected different moods with 8 out of 10 score. The wrong detections occured in Happy and Neutral mood where is misdetected it as Sad and Happy.\n",
    "\n",
    "Technique 3: 60%\n",
    "- This technique is the worst among the rest, it just correctly detected 6 out of 10 moods. It sometimes misdetected Happy with Sad and Sad with Happy. \n",
    "\n",
    "Overall Observation:\n",
    "\n",
    "- Techniques 1 and 2 are more consistent in their performance for detection different moods, while technique 3 needs a lot more improvement with the codes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef116e08-3c33-46dc-a5bd-613a50155f73",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfcc148-9c95-4125-a25c-965f16a7260e",
   "metadata": {},
   "source": [
    "## Summary and Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234d297-014b-4ab6-a427-627cf25b7632",
   "metadata": {},
   "source": [
    "This section must be answered concisely. Do not engage in unmeaningful writing for the summary and lessons learned. Provide your brief reflection only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577c344",
   "metadata": {},
   "source": [
    "- In this midterm examination, the students were able to apply the previously learned algorithms in order to recognize the data needed gathered by the web camera. Using computer vision, the students were able to do this by initially training a model in which the algorithm must have to recognize. Then, the main program will now open the web camera and recognize the objects which matches the understood model. This was observed on Task 1, 2, and 3 as it was all responsible in identifying the faces present in the computer's vision as well as recognizing them in the latter part of the examination. \n",
    "\n",
    "- On the third task, the students have used different techniques to implement mood detection on the live web camera. The students have utilized the previously learned algorithms which is to feed the data first in to a CSV file which then wille be used to detect the faces to be recognized. The students have used LBPHFaceRecognizer and EigenFaceRecognizer. For the independently learned algorithm, the students have not created a CSV file but instead, have the images on a directory and then the images will be appended to an array when the program is run. This array will serve as the basis of the algorithm to compare it to the real time capture of the web camera. \n",
    "\n",
    "- During the examination, the students were able to understood and apply the previously learned algorithms. The students were also able to modify the program in order for it to work properly as intended on the activity. The students believes that this examination is to apply the concepts about object and face detection, as well as recognition on other fields in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb275b8-d25b-4b4e-b1f3-e7ae48fd9de5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d81164a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=10RivbwakGWU2_ktf5W3F-KYNnYS_fq4m\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROOF OF COLLAB:\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(url='https://drive.google.com/uc?id=10RivbwakGWU2_ktf5W3F-KYNnYS_fq4m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9194e-b263-4d38-bfa3-ed6490825a43",
   "metadata": {},
   "source": [
    "**Proprietary Clause**\n",
    "\n",
    "Property of the Technological Institute of the Philippines (T.I.P.). No part of the materials made and uploaded in this learning management system by T.I.P. may be copied, photographed, printed, reproduced, shared, transmitted, translated or reduced to any electronic medium or machine-readable form, in whole or in part, without prior consent of T.I.P.\n",
    "\n",
    "Prepared by Engr. RMR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
